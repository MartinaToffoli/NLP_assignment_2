{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing: *Second assignment*** "
      ],
      "metadata": {
        "id": "lvwLIeL9-eAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Candidate: Martina Toffoli VR446059***"
      ],
      "metadata": {
        "id": "fqfSrzy97n6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt:**\n",
        "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a pipeline that, starting from a text in input, in a given language (English, French, German and Italian are admissible) outputs the syntactic tree of the sentence itself, intended as a tree with root in S for sentence, and leaves on the tokens labelled with a single Part-of-speech. The generation of the tree can pass through one of the following models:\n",
        "\n",
        "1) **PURE SYMBOLIC.** The tree is generated by a LR analysis with CF LL2 grammar as a base. Candidates can assume the following:\n",
        "\n",
        "    a) Adjectives in English and German shall be only prefixed to nouns, whilst in French and Italian are only suffixed;\n",
        "\n",
        "    b) Verbs are all at present tense;\n",
        "\n",
        "    c) No pronouns are admitted;\n",
        "\n",
        "    d) Only one adverb is admitted, always post-poned with respect to the verb (independently of the language, and the type of adverb);\n",
        "\n",
        "  Overall the point above map a system that could be devised in regular expressions, but a Context-free grammar would be simpler to     \n",
        "  define. Candidate can either define a system by themselves or use a syntactic tree generation system that can be found on GitHub. \n",
        "  Same happens for POS-tagging, where some of the above mentioned systems can be customized by existing techniques that are available\n",
        "  in several fashions (including a pre-defined NLTK and OpenNLP libraries for POS-tagging and a module in GATE for the same purpose. Ambiguity \n",
        "  should be blocked onto first admissible tree.\n",
        "\n",
        "2) **PURE ML.** Candidates can develop a PLM with one-step Markov chains to forecast the following token, and used to generate the forecast of the\n",
        "     POS tags to be attributed. In this case the PLM can be generated starting with a Corpus, that could be obtained online, for instance by \n",
        "     using the Wikipedia access API, or other available free repos (including those available with SketchEngine. In this approach, candidates should\n",
        "     never use the forecasting to approach the determination of outcomes (for this would be identical purpose of distinguishing EN/non ENG (and\n",
        "     then IT/non IT, FR/not FR or DE/not DE) but only to identify the POS model in a sequence. In this case, the candidate should output the most\n",
        "     likely POS tagging, without associating the sequence to a tree in a direct fashion.\n",
        "\n",
        "Candidates are free to employ PURE ML approach to simplify, or pre-process the text in order to improve the performance of a PURE SYMBOLIC approach while generating a mixed model."
      ],
      "metadata": {
        "id": "40d4ubnHeHT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation details:**\n",
        "The generation of the tree choosen is *PURE ML* model that which provided a way to use the spaCy pipeline. \n",
        "In this assignment, spaCy is used for tokenizazion and segmentation of phrases. \n",
        "\n",
        "But *What is spaCy?* \n",
        "\n",
        "SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python and  is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "For parsing is used benepar (Barkeley Neural Parser) because parser models do not ship with a tokenizer or sentence splitter, and some models may not include a part-of-speech tagger either.\n",
        "\n",
        "After all, a trivial model to print standard trees is to use a nltk tree-format.\n"
      ],
      "metadata": {
        "id": "t0QUu1e1fReI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To allow the software to run, we must first install the parser model benepar  using this command:"
      ],
      "metadata": {
        "id": "oro8fsaC4qxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install benepar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAcy0rsDoGjo",
        "outputId": "89f4cda4-41dc-4ea2-bc34-9853a6bacc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.8/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.8/dist-packages (from benepar) (3.4.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from benepar) (1.13.1+cu116)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from benepar) (3.19.6)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2->benepar) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2->benepar) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.2->benepar) (1.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (2.25.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (1.10.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (8.1.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->benepar) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=2.0.9->benepar) (2.0.1)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37646 sha256=4ad9e6e689b810200f7e0928754757f66fb09f4a955661029eaa405a91c56daa\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/a2/7e/f3631621bc53a802ee5a333d2b1bd0582fd50402cf702b3e37\n",
            "Successfully built benepar\n",
            "Installing collected packages: tokenizers, sentencepiece, torch-struct, huggingface-hub, transformers, benepar\n",
            "Successfully installed benepar-0.2.0 huggingface-hub-0.12.1 sentencepiece-0.1.97 tokenizers-0.13.2 torch-struct-0.5 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code:"
      ],
      "metadata": {
        "id": "s1yAYPxy5J-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import benepar, spacy\n",
        "import spacy.cli\n",
        "from nltk.corpus.europarl_raw import english, french, german, italian\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "from spacy.lang.en.examples import sentences as englishSenteces\n",
        "from spacy.lang.fr.examples import sentences as frenchSenteces\n",
        "from spacy.lang.de.examples import sentences as germanSenteces\n",
        "from spacy.lang.it.examples import sentences as italianSenteces\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('europarl_raw')\n",
        "\n",
        "benepar.download('benepar_en3')\n",
        "benepar.download('benepar_fr2')\n",
        "benepar.download('benepar_de2')\n",
        "benepar.download('benepar_it3')\n",
        "\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "spacy.cli.download('fr_core_news_sm')\n",
        "spacy.cli.download('de_core_news_sm')\n",
        "spacy.cli.download('it_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh9dkTh1ZG0V",
        "outputId": "e29b986d-53c4-46a8-a4a2-e41a0f7ab7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Package europarl_raw is already up-to-date!\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "[nltk_data] Downloading package benepar_fr2 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_fr2 is already up-to-date!\n",
            "[nltk_data] Downloading package benepar_de2 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_de2 is already up-to-date!\n",
            "[nltk_data] Error loading benepar_it3: Package 'benepar_it3' not found\n",
            "[nltk_data]     in index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENGLISH + Tree** "
      ],
      "metadata": {
        "id": "Sn767UVvqzcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
        "\n",
        "'''\n",
        "with europarl:\n",
        "englishPhrase = sent_tokenize(english.raw())[0]\n",
        "englishParsedString = list(nlp(englishPhrase).sents)[0]._.parse_string\n",
        "'''\n",
        "englishParsedString = list(nlp(englishSenteces[3]).sents)[0]._.parse_string\n",
        "englishTree = Tree.fromstring(englishParsedString)\n",
        "englishTree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAAYRiXVnFUI",
        "outputId": "bf62f1ab-8677-456a-f2c8-6d639db0a3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    S                                 \n",
            "   _________________|_______________________________   \n",
            "  |                 VP                              | \n",
            "  |      ___________|____________                   |  \n",
            "  |     |       |                PP                 | \n",
            "  |     |       |         _______|____              |  \n",
            "  NP    |       NP       |            NP            | \n",
            "  |     |    ___|___     |    ________|_______      |  \n",
            " NNP   VBZ  DT  JJ  NN   IN  DT      NNP     NNP    . \n",
            "  |     |   |   |   |    |   |        |       |     |  \n",
            "London  is  a  big city  in the     United Kingdom  . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FRENCH + Tree**"
      ],
      "metadata": {
        "id": "HAmuuCycq2uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('fr_core_news_sm')\n",
        "nlp.add_pipe('benepar', config={'model': 'benepar_fr2'})\n",
        "\n",
        "'''\n",
        "with europarl:\n",
        "#frenchPhrase = sent_tokenize(french.raw())[1]\n",
        "#frenchParsedString = list(nlp(frenchPhrase).sents)[0]._.parse_string\n",
        "'''\n",
        "frenchParsedString = list(nlp(frenchSenteces[1]).sents)[0]._.parse_string\n",
        "frenchTree = Tree.fromstring(frenchParsedString)\n",
        "frenchTree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOpkhoQvnOtW",
        "outputId": "d29888b3-fcc4-4826-c8a2-662821ec1b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          SENT                                                      \n",
            "        ___________________________________|____________________________________                     \n",
            "       |                   |                         NP                         |                   \n",
            "       |                   |       __________________|___                       |                    \n",
            "       NP                  |      |        |             PP                     PP                  \n",
            "  _____|_________          |      |        |          ___|___               ____|___                 \n",
            " |     |         AP        VN     |        |         |       NP            |        NP              \n",
            " |     |         |         |      |        |         |    ___|______       |     ___|________        \n",
            "DET    NC       ADJ        V     DET       NC        P  DET         NC     P   DET           NC     \n",
            " |     |         |         |      |        |         |   |          |      |    |            |       \n",
            "Les voitures autonomes déplacent  la responsabilité  de  l'     assurance vers les     constructeurs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GERMAN + Tree**"
      ],
      "metadata": {
        "id": "XCQYCPMntJXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('de_core_news_sm')\n",
        "nlp.add_pipe('benepar', config={'model': 'benepar_de2'})\n",
        "\n",
        "'''\n",
        "with europarl:\n",
        "germanPhrase = sent_tokenize(german.raw())[0]\n",
        "germanParsedString = list(nlp(germanPhrase).sents)[0]._.parse_string\n",
        "'''\n",
        "germanParsedString = list(nlp(germanSenteces[3]).sents)[0]._.parse_string\n",
        "germanTree = Tree.fromstring(germanParsedString)\n",
        "germanTree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALG6vix1nS3w",
        "outputId": "2fdda368-4ead-43fe-b978-77557e92578e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   S                                              \n",
            "        ___________|______________                                 \n",
            "       |           |              NP                              \n",
            "       |           |        ______|________                        \n",
            "       |           |       |               PP                     \n",
            "       |           |       |       ________|___________________    \n",
            "       NN        VVFIN     NN    APPR     ADJA        ADJA     NN \n",
            "       |           |       |      |        |           |       |   \n",
            "Bundesanwaltscha erhebt Anklage gegen mutmaßlichen Schweizer Spion\n",
            "       ft                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ITALIAN + Tree**"
      ],
      "metadata": {
        "id": "P78-8f0YtPP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('it_core_news_sm')\n",
        "nlp.add_pipe('benepar', config={'model': 'benepar_fr2'})\n",
        "\n",
        "'''\n",
        "with europarl:\n",
        "italianPhrase = sent_tokenize(italian.raw())[2]\n",
        "italianParsedString = list(nlp(italianPhrase).sents)[0]._.parse_string\n",
        "'''\n",
        "italianParsedString = list(nlp(italianSenteces[3]).sents)[0]._.parse_string\n",
        "italianTree = Tree.fromstring(italianParsedString)\n",
        "italianTree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQk3dNotnY-Z",
        "outputId": "4583ef94-0d3e-4cef-93a2-433bfff3c896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       SENT                           \n",
            "   _____________________|__________________________    \n",
            "  |     |               NP                         |  \n",
            "  |     |    ___________|_________                 |   \n",
            "  |     |   |    |      |         PP               |  \n",
            "  |     |   |    |      |     ____|____            |   \n",
            "  |     |   |    |      |    |         NP          |  \n",
            "  |     |   |    |      |    |         |           |   \n",
            "  NP    VN  |    |      |    |        NPP+         |  \n",
            "  |     |   |    |      |    |     ____|_____      |   \n",
            " NPP    V  DET  ADJ     NC  P+D  NPP        ADJ  PONCT\n",
            "  |     |   |    |      |    |    |          |     |   \n",
            "Londra  è  una grande città del Regno      Unito   .  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nkcd5IAs-aL9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dut1nlCRq13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}